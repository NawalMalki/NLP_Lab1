{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7889375,"sourceType":"datasetVersion","datasetId":4631748}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T23:40:40.645424Z","iopub.execute_input":"2024-03-19T23:40:40.645841Z","iopub.status.idle":"2024-03-19T23:40:41.118645Z","shell.execute_reply.started":"2024-03-19T23:40:40.645805Z","shell.execute_reply":"2024-03-19T23:40:41.117039Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/movie-review/movie_review.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importation des bibliothèques / fonctions nécessaires ","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport string \nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:40:41.121500Z","iopub.execute_input":"2024-03-19T23:40:41.122025Z","iopub.status.idle":"2024-03-19T23:40:41.998488Z","shell.execute_reply.started":"2024-03-19T23:40:41.121993Z","shell.execute_reply":"2024-03-19T23:40:41.997372Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Pré-traitement du texte ","metadata":{}},{"cell_type":"markdown","source":"#### Prétraitement du feature text","metadata":{}},{"cell_type":"code","source":"# Importing the dataset\nfile_path = '/kaggle/input/movie-review/movie_review.csv'\ndata = pd.read_csv(file_path)\n\n# Importing the columns I wanna work with\nText = data['text']\nTag = data['tag']\n\n# Creating a new dataframe where I'll put my work \nPreprocessing_data = data[['text', 'tag']].copy()\n\n#### Step 1: Normalize #####\nPreprocessing_data['text'] = data['text'].str.lower()\n\n#### Step 2: Remove stop words and tokenize ####\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords_and_tokenize(text):\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words and not word.startswith('@')]\n    return ' '.join(filtered_text)\n\nPreprocessing_data['text'] = Preprocessing_data['text'].apply(remove_stopwords_and_tokenize)\n\n#### Step 3: Stem ####\nstemmer = PorterStemmer()\ndef stem_text(text):\n    stemmed_text = [stemmer.stem(word) for word in word_tokenize(text)]\n    return stemmed_text\n\nPreprocessing_data['text'] = Preprocessing_data['text'].apply(stem_text)\n\n# Preprocessed data \nprint(Preprocessing_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:40:42.000076Z","iopub.execute_input":"2024-03-19T23:40:42.000743Z","iopub.status.idle":"2024-03-19T23:41:50.133328Z","shell.execute_reply.started":"2024-03-19T23:40:42.000699Z","shell.execute_reply":"2024-03-19T23:41:50.132165Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"                                                    text  tag\n0      [film, adapt, comic, book, plenti, success, ,,...  pos\n1      [starter, ,, creat, alan, moor, (, eddi, campb...  pos\n2      [say, moor, campbel, thoroughli, research, sub...  pos\n3      [book, (, ``, graphic, novel, ,, ``, ), 500, p...  pos\n4                [word, ,, n't, dismiss, film, sourc, .]  pos\n...                                                  ...  ...\n64715   [lack, inspir, trace, back, insipid, charact, .]  neg\n64716  [like, mani, skit, current, incarn, _saturday_...  neg\n64717  [watch, one, ``, roxburi, ``, skit, snl, ,, co...  neg\n64718        [bump, unsuspect, women, ,, ., ., ., 's, .]  neg\n64719  [watch, _a_night_at_the_roxbury_, ,, 'll, left...  neg\n\n[64720 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Prétraitement du target tag (Convertir les valeurs catégorielles en valeurs numériques)","metadata":{}},{"cell_type":"code","source":"# Transformer les valeurs de la colonne 'tag' en valeurs numériques\nle = LabelEncoder()\nPreprocessing_data['tag'] = le.fit_transform(Preprocessing_data['tag'])\n\n# Afficher les données prétraitées\nPreprocessing_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:41:50.136381Z","iopub.execute_input":"2024-03-19T23:41:50.136853Z","iopub.status.idle":"2024-03-19T23:41:50.173724Z","shell.execute_reply.started":"2024-03-19T23:41:50.136802Z","shell.execute_reply":"2024-03-19T23:41:50.172402Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text  tag\n0  [film, adapt, comic, book, plenti, success, ,,...    1\n1  [starter, ,, creat, alan, moor, (, eddi, campb...    1\n2  [say, moor, campbel, thoroughli, research, sub...    1\n3  [book, (, ``, graphic, novel, ,, ``, ), 500, p...    1\n4            [word, ,, n't, dismiss, film, sourc, .]    1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[film, adapt, comic, book, plenti, success, ,,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[starter, ,, creat, alan, moor, (, eddi, campb...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[say, moor, campbel, thoroughli, research, sub...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[book, (, ``, graphic, novel, ,, ``, ), 500, p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[word, ,, n't, dismiss, film, sourc, .]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Entraînement du modèle Word2Vec","metadata":{}},{"cell_type":"code","source":"import gensim\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec\n\n# Tokenizer les données\ntokenized_data = Preprocessing_data['text'].tolist()\n\n# Construire le modèle Word2Vec\nmodel = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n\n# Entraîner le modèle Word2Vec\nmodel.train(tokenized_data, total_examples=len(tokenized_data), epochs=10)\n\n# Sauvegarder le modèle\nmodel.save(\"modele_word2vec.model\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:41:50.175230Z","iopub.execute_input":"2024-03-19T23:41:50.176530Z","iopub.status.idle":"2024-03-19T23:42:19.360207Z","shell.execute_reply.started":"2024-03-19T23:41:50.176486Z","shell.execute_reply":"2024-03-19T23:42:19.358922Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Vectorisation des reviews des movies ","metadata":{}},{"cell_type":"code","source":"# Fonction pour obtenir l'embedding moyen des mots d'un commentaire\ndef get_average_word_embedding(commentaire, model, taille_vecteur):\n    # Tokeniser le commentaire\n    tokens = word_tokenize(commentaire)\n    # Obtenir les embeddings de chaque mot dans le commentaire\n    embeddings = [model.wv[mot] for mot in tokens if mot in model.wv]\n    if embeddings:\n        # Calculer l'embedding moyen\n        return np.mean(embeddings, axis=0)\n    else:\n        # Retourner des zéros si aucun embedding n'est trouvé\n        return np.zeros(taille_vecteur)\n\n# Vectoriser les commentaires\ntaille_vecteur = model.vector_size\nPreprocessing_data['vectorized_text'] = Preprocessing_data['text'].astype(str).apply(lambda x: get_average_word_embedding(x, model, taille_vecteur))\n\n# Afficher les données vectorisées\nprint(Preprocessing_data['vectorized_text'])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:42:19.362983Z","iopub.execute_input":"2024-03-19T23:42:19.363334Z","iopub.status.idle":"2024-03-19T23:43:02.236076Z","shell.execute_reply.started":"2024-03-19T23:42:19.363303Z","shell.execute_reply":"2024-03-19T23:43:02.233059Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0        [-1.1120574, 0.5487967, -0.84684944, -0.460104...\n1        [-1.1345445, 0.5565828, -0.8483506, -0.4552571...\n2        [-1.1278533, 0.5140112, -0.9623763, -0.4581380...\n3        [-1.2645186, 0.5945389, -0.9256986, -0.5126380...\n4        [-1.0650451, 0.47696635, -0.58653253, -0.20748...\n                               ...                        \n64715    [-1.100257, 0.47778407, -0.8095651, -0.4457729...\n64716    [-1.0771482, 0.49123654, -0.88001746, -0.41208...\n64717    [-1.1907177, 0.530403, -0.9080313, -0.44727468...\n64718    [-1.1888479, 0.45145863, -0.6661015, -0.363230...\n64719    [-1.076462, 0.47772917, -0.597988, -0.28583235...\nName: vectorized_text, Length: 64720, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Division du dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Séparer les features (vecteurs de mots) et les labels\nX = Preprocessing_data['vectorized_text'].tolist()\ny = Preprocessing_data['tag'].tolist()\n\n# Diviser le dataset en ensembles d'entraînement et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:43:02.237557Z","iopub.execute_input":"2024-03-19T23:43:02.237936Z","iopub.status.idle":"2024-03-19T23:43:02.285723Z","shell.execute_reply.started":"2024-03-19T23:43:02.237906Z","shell.execute_reply":"2024-03-19T23:43:02.284565Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Implémentation du modèle de régression logistique","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Mettre à l'échelle les données\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Créer et entraîner le modèle de régression logistique avec le solveur 'saga'\nlog_reg = LogisticRegression(max_iter=8000, solver='saga')\nlog_reg.fit(X_train_scaled, y_train)\n\n# Prédire les labels pour l'ensemble de test\ny_pred = log_reg.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:43:02.287274Z","iopub.execute_input":"2024-03-19T23:43:02.287622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calcul des métriques d'évaluation","metadata":{}},{"cell_type":"code","source":"# Calculer les métriques d'évaluation\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf2_score = f1_score(y_test, y_pred, average='weighted')\n\n# Afficher les résultats\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F2 Score: {f2_score}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}