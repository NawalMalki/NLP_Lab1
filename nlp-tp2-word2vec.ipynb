{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7828554,"sourceType":"datasetVersion","datasetId":4587752}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Entraînement du modèle Word2Vec","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lecture du texte : ","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport string\nimport random\n\n# Lecture du texte à partir d'un fichier\nwith open('/kaggle/input/dataset/text', 'r', encoding='utf-8') as file:\n    texte = file.read()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T22:23:33.315625Z","iopub.execute_input":"2024-03-12T22:23:33.316054Z","iopub.status.idle":"2024-03-12T22:23:33.322966Z","shell.execute_reply.started":"2024-03-12T22:23:33.316022Z","shell.execute_reply":"2024-03-12T22:23:33.321378Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Prétraitement du texte :","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n# Prétraitement du texte : tokenization, suppression de la ponctuation et des stopwords\ntokens = word_tokenize(texte.lower())\ntokens = [token for token in tokens if token.isalpha()]\ntokens = [token for token in tokens if token not in stopwords.words('french')]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T22:23:53.290773Z","iopub.execute_input":"2024-03-12T22:23:53.291597Z","iopub.status.idle":"2024-03-12T22:23:53.355780Z","shell.execute_reply.started":"2024-03-12T22:23:53.291557Z","shell.execute_reply":"2024-03-12T22:23:53.354780Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Entraînement du modèle Word2Vec","metadata":{}},{"cell_type":"code","source":"modele = Word2Vec([tokens], vector_size=100, window=5, min_count=1, workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T22:24:30.177116Z","iopub.execute_input":"2024-03-12T22:24:30.177584Z","iopub.status.idle":"2024-03-12T22:24:30.205571Z","shell.execute_reply.started":"2024-03-12T22:24:30.177549Z","shell.execute_reply":"2024-03-12T22:24:30.204334Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Opérations Word2Vec sur des mots aléatoires du texte","metadata":{}},{"cell_type":"code","source":"# Sélection aléatoire de deux mots du texte pour les opérations Word2Vec\nmots_aleatoires = random.sample(tokens, 2)\nmot1, mot2 = mots_aleatoires\n\n# Extraction de la représentation vectorielle d'un mot\nvector = modele.wv[mot1]\nprint(f\"Représentation vectorielle du mot '{mot1}': {vector}\")\n\n# Calcul de la similarité entre deux mots\nsimilarite = modele.wv.similarity(mot1, mot2)\nprint(f\"Similarité entre '{mot1}' et '{mot2}': {similarite}\")\n\n# Extraction des mots contextuels (les plus similaires) pour un mot central donné\nmot_central = mot1\nmots_contextuels = modele.wv.most_similar(mot_central, topn=3)\nprint(f\"Mots contextuels pour le mot central :'{mot_central}': {mots_contextuels}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T22:26:19.537659Z","iopub.execute_input":"2024-03-12T22:26:19.538162Z","iopub.status.idle":"2024-03-12T22:26:19.551820Z","shell.execute_reply.started":"2024-03-12T22:26:19.538124Z","shell.execute_reply":"2024-03-12T22:26:19.550280Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Représentation vectorielle du mot 'is': [ 8.0021834e-03 -4.3858918e-03 -1.1576273e-03  1.1058913e-03\n -7.0482012e-05  9.0490369e-04  6.0227010e-03  4.6281499e-04\n -3.4172805e-03 -1.6677473e-03  5.7483884e-03  1.2845262e-03\n -9.5808035e-04  9.4504766e-03 -4.9685161e-03 -1.0672822e-03\n  9.3054501e-03  6.5573137e-03  1.4595756e-03 -9.2741428e-03\n  1.0836259e-03 -2.2643399e-03  9.5151439e-03  1.1426334e-03\n  1.6058169e-03  2.4247789e-03 -1.9416843e-03 -4.9809283e-03\n -4.8344427e-05 -1.8281313e-03  6.8531921e-03  9.0147583e-03\n -6.0303684e-04  2.9097628e-03 -6.2481193e-03  2.0658521e-03\n -6.9334214e-03 -8.9138169e-03 -6.0883574e-03 -9.2516821e-03\n  7.2933161e-03 -6.0871099e-03  8.1293425e-03 -7.2193658e-03\n  3.5822573e-03  9.6523361e-03 -7.7956663e-03 -9.9888621e-03\n -4.2304238e-03 -2.6215445e-03 -9.3350485e-05 -8.9580007e-03\n -8.4491950e-03  2.7515984e-03 -8.3661573e-03 -8.8428073e-03\n -2.3201422e-03 -8.7422682e-03 -7.2597875e-03 -8.3523327e-03\n -1.6974892e-04 -4.4384091e-03  6.6689984e-03  1.4641391e-03\n -3.6935390e-03  6.2528811e-03 -5.7813413e-03 -4.4064275e-03\n -7.5167618e-03 -4.0134080e-03 -1.9672080e-03  6.6192937e-03\n -2.5793738e-03  4.8453226e-03  7.1135308e-03 -7.1302294e-03\n  4.6704612e-03  6.1293188e-03 -3.2594453e-03  6.6955294e-03\n  6.1299508e-03 -6.3957251e-03 -6.9696852e-03  2.8496906e-03\n -1.7841252e-03 -6.0409401e-03  9.5636146e-03 -4.8821736e-03\n -6.3748071e-03 -2.2007886e-04 -2.3749007e-03  6.4991741e-04\n -3.4895539e-03 -4.1268687e-04 -2.6966754e-04  1.0122190e-03\n  8.3740121e-03 -5.8719898e-03 -1.5431056e-03  5.4402109e-03]\nSimilarité entre 'is' et 'distinctive': 0.07642822712659836\nMots contextuels pour le mot central :'is': [('has', 0.3573882579803467), ('north', 0.3026951551437378), ('every', 0.2874441146850586)]\n","output_type":"stream"}]}]}