{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8841,"sourceType":"datasetVersion","datasetId":4133}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-18T15:22:46.219076Z","iopub.execute_input":"2024-02-18T15:22:46.219882Z","iopub.status.idle":"2024-02-18T15:22:47.011394Z","shell.execute_reply.started":"2024-02-18T15:22:46.219841Z","shell.execute_reply":"2024-02-18T15:22:47.010563Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/customer-support-on-twitter/sample.csv\n/kaggle/input/customer-support-on-twitter/twcs/twcs.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd \nimport string \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\n# Importing the sample.csv file \nfile_path = '/kaggle/input/customer-support-on-twitter/sample.csv'\ndata = pd.read_csv(file_path)\n\n# Importing the columns I wanna work with\nTweet_id = data['tweet_id']\nText = data['text']\n\n# Creating a new dataframe where I'll put my work \nPreprocessing_data = data[['tweet_id', 'text']].copy()\n\n#### Step 1: Normalize #####\nPreprocessing_data['text_cleaned'] = data['text'].str.lower()\n\n#### Step 2: Remove stop words and tokenize ####\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords_and_tokenize(text):\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words and not word.startswith('@')]\n    return ' '.join(filtered_text)\n\nPreprocessing_data['text_cleaned'] = Preprocessing_data['text_cleaned'].apply(remove_stopwords_and_tokenize)\n\n#### Step 3: Stem ####\nstemmer = PorterStemmer()\ndef stem_text(text):\n    stemmed_text = [stemmer.stem(word) for word in word_tokenize(text)]\n    return stemmed_text\n\nPreprocessing_data['text_cleaned'] = Preprocessing_data['text_cleaned'].apply(stem_text)\n\n# Preprocessed data \nprint(Preprocessing_data)\n\n# Save the new dataframe into my notebook\nPreprocessing_data.to_csv('preprocessed_data.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T16:30:21.689436Z","iopub.execute_input":"2024-02-18T16:30:21.689725Z","iopub.status.idle":"2024-02-18T16:30:21.779421Z","shell.execute_reply.started":"2024-02-18T16:30:21.689704Z","shell.execute_reply":"2024-02-18T16:30:21.778582Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"    tweet_id                                               text  \\\n0     119237  @AppleSupport causing the reply to be disregar...   \n1     119238  @105835 Your business means a lot to us. Pleas...   \n2     119239  @76328 I really hope you all change but I'm su...   \n3     119240  @105836 LiveChat is online at the moment - htt...   \n4     119241  @VirginTrains see attached error message. I've...   \n..       ...                                                ...   \n88    119330  @105860 I wish Amazon had an option of where I...   \n89    119331  They reschedule my shit for tomorrow https://t...   \n90    119332  @105861 Hey Sara, sorry to hear of the issues ...   \n91    119333  @Tesco bit of both - finding the layout cumber...   \n92    119335  @105861 If that doesn't help please DM your fu...   \n\n                                         text_cleaned  \n0   [applesupport, caus, repli, disregard, tap, no...  \n1   [105835, busi, mean, lot, us, ., pleas, dm, na...  \n2   [76328, realli, hope, chang, 'm, sure, wo, n't...  \n3   [105836, livechat, onlin, moment, -, http, :, ...  \n4   [virgintrain, see, attach, error, messag, ., '...  \n..                                                ...  \n88  [105860, wish, amazon, option, get, ship, up, ...  \n89  [reschedul, shit, tomorrow, http, :, //t.co/rs...  \n90  [105861, hey, sara, ,, sorri, hear, issu, ,, a...  \n91  [tesco, bit, -, find, layout, cumbersom, remov...  \n92  [105861, n't, help, pleas, dm, full, name, ,, ...  \n\n[93 rows x 3 columns]\n","output_type":"stream"}]}]}